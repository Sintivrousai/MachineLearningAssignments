{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db412b-7e24-4126-a735-9cb1de723aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install tiktoken\n",
    "!pip install sentencepiece==0.1.96\n",
    "!pip install focal-loss-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4821ee0-2ac1-4d7e-8d79-f991488b0124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18a1d527-df07-4bad-8369-1ecf4f3b4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, DebertaV2Tokenizer, DebertaV2Model,DebertaV2ForSequenceClassification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tiktoken\n",
    "from transformers import DebertaV2Tokenizer\n",
    "import sentencepiece\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from focal_loss import FocalLoss\n",
    "\n",
    "\n",
    "# Load your dataset (adjust file path)\n",
    "df = pd.read_csv(\"Data/incidents_train.csv\", encoding='utf-8')  # Replace with your file path\n",
    "\n",
    "# Drop missing values\n",
    "df = df.dropna(subset=[\"title\", \"product-category\", \"hazard-category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51c2ef08-f5b8-4bb9-a67c-c10ef99978f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before applying class weights (Product Category):\n",
      "13    1149\n",
      "1      536\n",
      "9      428\n",
      "18     375\n",
      "19     214\n",
      "20     211\n",
      "15     210\n",
      "12     178\n",
      "2      168\n",
      "3      136\n",
      "14     106\n",
      "4      104\n",
      "10     103\n",
      "0       47\n",
      "16      44\n",
      "5       15\n",
      "17      15\n",
      "7        7\n",
      "11       6\n",
      "6        5\n",
      "8        4\n",
      "21       4\n",
      "Name: product-category, dtype: int64\n",
      "\n",
      "Class distribution before applying class weights (Hazard Category):\n",
      "0    1485\n",
      "1    1390\n",
      "4     449\n",
      "5     298\n",
      "2     232\n",
      "8     106\n",
      "9      43\n",
      "7      41\n",
      "3      19\n",
      "6       2\n",
      "Name: hazard-category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to integers (for classification)\n",
    "df[\"product-category\"] = df[\"product-category\"].astype(\"category\").cat.codes\n",
    "df[\"hazard-category\"] = df[\"hazard-category\"].astype(\"category\").cat.codes\n",
    "\n",
    "\n",
    "# Create stratification label by combining product and hazard categories\n",
    "df[\"stratify_label\"] = df[\"product-category\"].astype(str) + \"_\" + df[\"hazard-category\"].astype(str)\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Get train-test split indices\n",
    "for train_index, test_index in skf.split(df[\"title\"], df[\"stratify_label\"]):\n",
    "    train_df = df.iloc[train_index]\n",
    "    test_df = df.iloc[test_index]\n",
    "    break  # Use the first fold as train-test split\n",
    "\n",
    "# Extract final train and test sets\n",
    "X_train, X_test = train_df[\"title\"], test_df[\"title\"]\n",
    "y_train_product, y_test_product = train_df[\"product-category\"], test_df[\"product-category\"]\n",
    "y_train_hazard, y_test_hazard = train_df[\"hazard-category\"], test_df[\"hazard-category\"]\n",
    "# Check class distribution in the training set before applying weights\n",
    "print(\"Class distribution before applying class weights (Product Category):\")\n",
    "print(y_train_product.value_counts())\n",
    "\n",
    "print(\"\\nClass distribution before applying class weights (Hazard Category):\")\n",
    "print(y_train_hazard.value_counts())\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"  # Change to DeBERTa Large\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = DebertaV2Model.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move tokenized input to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def tokenize_text(texts):\n",
    "    encodings = tokenizer(list(texts), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    return encodings.to(device)\n",
    "\n",
    "train_encodings = tokenize_text(X_train)\n",
    "test_encodings = tokenize_text(X_test)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels_product = torch.tensor(y_train_product.values)\n",
    "train_labels_hazard = torch.tensor(y_train_hazard.values)\n",
    "test_labels_product = torch.tensor(y_test_product.values)\n",
    "test_labels_hazard = torch.tensor(y_test_hazard.values)\n",
    "\n",
    "# Compute Class Weights for the resampled data\n",
    "product_class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train_product), y=y_train_product)\n",
    "hazard_class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train_hazard), y=y_train_hazard)\n",
    "\n",
    "# Convert class weights to tensors and move them to GPU if available\n",
    "product_class_weights = torch.tensor(product_class_weights, dtype=torch.float).to(device)\n",
    "hazard_class_weights = torch.tensor(hazard_class_weights, dtype=torch.float).to(device)\n",
    "# Convert Pandas Series to NumPy arrays for proper indexing\n",
    "y_train_product_np = y_train_product.to_numpy()\n",
    "y_train_hazard_np = y_train_hazard.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddd65b80-60f7-4cee-9a14-311d84286fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "class ProductDeBERTa(nn.Module):\n",
    "    def __init__(self, model_name, num_product_labels):\n",
    "        super(ProductDeBERTa, self).__init__()\n",
    "        self.deberta = DebertaV2Model.from_pretrained(model_name)\n",
    "        self.product_classifier = nn.Linear(self.deberta.config.hidden_size, num_product_labels)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # DeBERTa outputs a BaseModelOutput object\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # Extract last hidden state\n",
    "        cls_hidden_state = hidden_states[:, 0, :]  # Get [CLS] token representation\n",
    "        cls_hidden_state = self.dropout(cls_hidden_state)  # Apply dropout\n",
    "        product_logits = self.product_classifier(cls_hidden_state)  # Apply classifier on [CLS] token\n",
    "        return product_logits  # Return the logits directly\n",
    "        \n",
    "class HazardDeBERTa(nn.Module):\n",
    "    def __init__(self, model_name, num_hazard_labels):\n",
    "        super(HazardDeBERTa, self).__init__()\n",
    "        self.deberta = DebertaV2Model.from_pretrained(model_name)\n",
    "        self.hazard_classifier = nn.Linear(self.deberta.config.hidden_size, num_hazard_labels)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        cls_hidden_state = hidden_states[:, 0, :]  # [CLS] token\n",
    "        cls_hidden_state = self.dropout(cls_hidden_state)\n",
    "        hazard_logits = self.hazard_classifier(cls_hidden_state)\n",
    "        return hazard_logits\n",
    "# Get number of classes for each task\n",
    "num_product_labels = len(df[\"product-category\"].unique())\n",
    "num_hazard_labels = len(df[\"hazard-category\"].unique())\n",
    "\n",
    "# Initialize both models separately\n",
    "product_model = ProductDeBERTa(MODEL_NAME, num_product_labels)\n",
    "hazard_model = HazardDeBERTa(MODEL_NAME, num_hazard_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c24d5568-b94a-4de5-b8a4-843cc5e2883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Create Dataset for product task\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, encodings, labels_product):\n",
    "        self.encodings = encodings\n",
    "        self.labels_product = labels_product\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_product)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels_product\"] = self.labels_product[idx]\n",
    "        return item\n",
    "\n",
    "# Create Dataset for hazard task\n",
    "class HazardDataset(Dataset):\n",
    "    def __init__(self, encodings, labels_hazard):\n",
    "        self.encodings = encodings\n",
    "        self.labels_hazard = labels_hazard\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_hazard)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels_hazard\"] = self.labels_hazard[idx]\n",
    "        return item\n",
    "\n",
    "# Create Dataset for product classification task\n",
    "product_train_dataset = ProductDataset(train_encodings, train_labels_product)\n",
    "product_test_dataset = ProductDataset(test_encodings, test_labels_product)\n",
    "\n",
    "# Create Dataset for hazard classification task\n",
    "hazard_train_dataset = HazardDataset(train_encodings, train_labels_hazard)\n",
    "hazard_test_dataset = HazardDataset(test_encodings, test_labels_hazard)\n",
    "\n",
    "# Create DataLoader for the product classification task\n",
    "product_train_loader = DataLoader(product_train_dataset, batch_size=16, shuffle=True)\n",
    "product_test_loader = DataLoader(product_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Create DataLoader for the hazard classification task\n",
    "hazard_train_loader = DataLoader(hazard_train_dataset, batch_size=16, shuffle=True)\n",
    "hazard_test_loader = DataLoader(hazard_test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a66b4585-13a1-44b8-a0d5-beb51b7244de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_930/896477393.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Product Model Loss: 1.8739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_930/896477393.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Hazard Model Loss: 0.7347\n",
      "Epoch 2 | Product Model Loss: 1.0483\n",
      "Epoch 2 | Hazard Model Loss: 0.3920\n",
      "Epoch 3 | Product Model Loss: 0.7004\n",
      "Epoch 3 | Hazard Model Loss: 0.2956\n",
      "Epoch 4 | Product Model Loss: 0.5175\n",
      "Epoch 4 | Hazard Model Loss: 0.2291\n",
      "Epoch 5 | Product Model Loss: 0.4358\n",
      "Epoch 5 | Hazard Model Loss: 0.1813\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming product_class_weights and hazard_class_weights are tensors\n",
    "loss_fn_product = FocalLoss(gamma=2, reduction='mean')\n",
    "loss_fn_hazard = FocalLoss(gamma=2, reduction='mean')\n",
    "\n",
    "# Separate optimizers for product and hazard models\n",
    "optimizer_product = AdamW(product_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "optimizer_hazard = AdamW(hazard_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler for both models (if necessary)\n",
    "num_training_steps_product = len(product_train_loader) * 5  # 5 epochs for product model\n",
    "num_training_steps_hazard = len(hazard_train_loader) * 5  # 5 epochs for hazard model\n",
    "lr_scheduler_product = get_scheduler(\"linear\", optimizer=optimizer_product, num_warmup_steps=0, num_training_steps=num_training_steps_product)\n",
    "lr_scheduler_hazard = get_scheduler(\"linear\", optimizer=optimizer_hazard, num_warmup_steps=0, num_training_steps=num_training_steps_hazard)\n",
    "\n",
    "product_model = product_model.to(device)\n",
    "hazard_model = hazard_model.to(device)\n",
    "\n",
    "# Training Loop for both models\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Train Product Model\n",
    "    product_model.train()\n",
    "    total_loss_product = 0\n",
    "    for batch in product_train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}  # Move batch items to the correct device\n",
    "\n",
    "        optimizer_product.zero_grad()  # Zero gradients for product model\n",
    "\n",
    "        # Forward pass for product model\n",
    "        product_logits = product_model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "\n",
    "        product_probs = F.softmax(product_logits, dim=-1)  # For product classification\n",
    "\n",
    "        # Ensure labels are in LongTensor format\n",
    "        batch[\"labels_product\"] = batch[\"labels_product\"].to(device).long()\n",
    "\n",
    "        # Compute loss for product model\n",
    "        loss_product = loss_fn_product(product_probs, batch[\"labels_product\"])\n",
    "\n",
    "        # Backpropagation for product model\n",
    "        loss_product.backward()\n",
    "        optimizer_product.step()\n",
    "\n",
    "        # Step learning rate scheduler\n",
    "        lr_scheduler_product.step()\n",
    "\n",
    "        total_loss_product += loss_product.item()\n",
    "\n",
    "    # Average loss for product model after each epoch\n",
    "    avg_loss_product = total_loss_product / len(product_train_loader)\n",
    "    print(f\"Epoch {epoch+1} | Product Model Loss: {avg_loss_product:.4f}\")\n",
    "\n",
    "    # Train Hazard Model\n",
    "    hazard_model.train()\n",
    "    total_loss_hazard = 0\n",
    "    for batch in hazard_train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}  # Move batch items to the correct device\n",
    "\n",
    "        optimizer_hazard.zero_grad()  # Zero gradients for hazard model\n",
    "\n",
    "        # Forward pass for hazard model\n",
    "        hazard_logits = hazard_model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "\n",
    "        hazard_probs = F.softmax(hazard_logits, dim=-1)    # For hazard classification\n",
    "\n",
    "        # Ensure labels are in LongTensor format\n",
    "        batch[\"labels_hazard\"] = batch[\"labels_hazard\"].to(device).long()\n",
    "\n",
    "        # Compute loss for hazard model\n",
    "        loss_hazard = loss_fn_hazard(hazard_probs, batch[\"labels_hazard\"])\n",
    "\n",
    "        # Backpropagation for hazard model\n",
    "        loss_hazard.backward()\n",
    "        optimizer_hazard.step()\n",
    "\n",
    "        # Step learning rate scheduler\n",
    "        lr_scheduler_hazard.step()\n",
    "\n",
    "        total_loss_hazard += loss_hazard.item()\n",
    "\n",
    "    # Average loss for hazard model after each epoch\n",
    "    avg_loss_hazard = total_loss_hazard / len(hazard_train_loader)\n",
    "    print(f\"Epoch {epoch+1} | Hazard Model Loss: {avg_loss_hazard:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09c93732-6d20-471f-a0c0-0d10c9ae22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_930/896477393.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Category Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.60      0.55        15\n",
      "           1       0.70      0.75      0.72       155\n",
      "           2       0.61      0.79      0.69        48\n",
      "           3       0.54      0.41      0.46        37\n",
      "           4       0.53      0.51      0.52        35\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00         4\n",
      "           9       0.68      0.73      0.70       133\n",
      "          10       0.50      0.63      0.56        35\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.90      0.80      0.85        46\n",
      "          13       0.86      0.91      0.88       339\n",
      "          14       0.81      0.61      0.70        36\n",
      "          15       0.68      0.77      0.72        65\n",
      "          16       0.00      0.00      0.00        12\n",
      "          17       0.00      0.00      0.00         4\n",
      "          18       0.60      0.49      0.53       103\n",
      "          19       0.85      0.88      0.87        60\n",
      "          20       0.76      0.76      0.76        59\n",
      "          21       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.73      1197\n",
      "   macro avg       0.43      0.44      0.43      1197\n",
      "weighted avg       0.72      0.73      0.72      1197\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipykernel_930/896477393.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hazard Category Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86       392\n",
      "           1       0.81      0.87      0.84       403\n",
      "           2       0.65      0.64      0.65       100\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.61      0.66      0.64       155\n",
      "           5       0.81      0.57      0.67        82\n",
      "           6       0.00      0.00      0.00         3\n",
      "           7       0.80      0.36      0.50        11\n",
      "           8       0.45      0.34      0.39        29\n",
      "           9       1.00      0.25      0.40        16\n",
      "\n",
      "    accuracy                           0.78      1197\n",
      "   macro avg       0.60      0.46      0.49      1197\n",
      "weighted avg       0.77      0.78      0.77      1197\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_product_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds_product = []\n",
    "    all_labels_product = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            product_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            preds_product = torch.argmax(product_logits, dim=1).cpu().numpy()\n",
    "            all_preds_product.extend(preds_product)\n",
    "            all_labels_product.extend(batch[\"labels_product\"].cpu().numpy())\n",
    "    \n",
    "    print(\"Product Category Classification Report:\")\n",
    "    print(classification_report(all_labels_product, all_preds_product))\n",
    "    return all_labels_product, all_preds_product\n",
    "\n",
    "def evaluate_hazard_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds_hazard = []\n",
    "    all_labels_hazard = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            hazard_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            preds_hazard = torch.argmax(hazard_logits, dim=1).cpu().numpy()\n",
    "            all_preds_hazard.extend(preds_hazard)\n",
    "            all_labels_hazard.extend(batch[\"labels_hazard\"].cpu().numpy())\n",
    "    \n",
    "    print(\"Hazard Category Classification Report:\")\n",
    "    print(classification_report(all_labels_hazard, all_preds_hazard))\n",
    "    return all_labels_hazard, all_preds_hazard\n",
    "\n",
    "# Evaluate the product and hazard models separately\n",
    "y_test_product, all_preds_product = evaluate_product_model(product_model, product_test_loader)\n",
    "y_test_hazard, all_preds_hazard = evaluate_hazard_model(hazard_model, hazard_test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ff057d-e271-4bb0-a99b-0979776621ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom F1 Score: 0.5079\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_score(hazards_true, products_true, hazards_pred, products_pred):\n",
    "    # Convert to NumPy arrays for Boolean indexing\n",
    "    hazards_true = np.array(hazards_true)\n",
    "    products_true = np.array(products_true)\n",
    "    hazards_pred = np.array(hazards_pred)\n",
    "    products_pred = np.array(products_pred)\n",
    "\n",
    "    # Compute F1-score for hazards\n",
    "    f1_hazards = f1_score(hazards_true, hazards_pred, average='macro')\n",
    "\n",
    "    # Compute F1-score for products (only where hazard prediction is correct)\n",
    "    correct_hazard_mask = hazards_pred == hazards_true\n",
    "    f1_products = f1_score(\n",
    "        products_true[correct_hazard_mask], \n",
    "        products_pred[correct_hazard_mask], \n",
    "        average='macro'\n",
    "    ) if np.any(correct_hazard_mask) else 0  # Handle case where no correct hazards\n",
    "\n",
    "    # Compute final score\n",
    "    return (f1_hazards + f1_products) / 2.\n",
    "\n",
    "score = compute_score(y_test_hazard, y_test_product, all_preds_hazard, all_preds_product)\n",
    "print(f\"Custom F1 Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18a5fca9-00a7-4c34-81e8-f3ad895fa79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_930/896477393.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/tmp/ipykernel_930/896477393.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom F1 Score on New Test Data: 0.4036\n"
     ]
    }
   ],
   "source": [
    "# Load new test file (adjust file path)\n",
    "new_test_df = pd.read_csv(\"Data/incidents_test.csv\")  # Replace with your actual file\n",
    "\n",
    "# Ensure it has the same preprocessing\n",
    "new_test_df = new_test_df.dropna(subset=[\"title\", \"product-category\", \"hazard-category\"])\n",
    "new_test_df[\"product-category\"] = new_test_df[\"product-category\"].astype(\"category\").cat.codes\n",
    "new_test_df[\"hazard-category\"] = new_test_df[\"hazard-category\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Tokenize new test set\n",
    "new_test_encodings = tokenize_text(new_test_df[\"title\"])\n",
    "\n",
    "# Convert labels to tensors\n",
    "new_test_labels_product = torch.tensor(new_test_df[\"product-category\"].values)\n",
    "new_test_labels_hazard = torch.tensor(new_test_df[\"hazard-category\"].values)\n",
    "\n",
    "# Create Dataset for product classification task\n",
    "product_new_test_dataset = ProductDataset(new_test_encodings, new_test_labels_product)\n",
    "\n",
    "# Create Dataset for hazard classification task\n",
    "hazard_new_test_dataset = HazardDataset(new_test_encodings, new_test_labels_hazard)\n",
    "\n",
    "# Create DataLoader for the product classification task\n",
    "product_new_test_loader = DataLoader(product_new_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Create DataLoader for the hazard classification task\n",
    "hazard_new_test_loader = DataLoader(hazard_new_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Evaluate the product model\n",
    "def evaluate_product_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds_product = []\n",
    "    all_labels_product = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            product_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            preds_product = torch.argmax(product_logits, dim=1).cpu().numpy()\n",
    "            all_preds_product.extend(preds_product)\n",
    "            all_labels_product.extend(batch[\"labels_product\"].cpu().numpy())\n",
    "    \n",
    "    return all_labels_product, all_preds_product\n",
    "\n",
    "# Evaluate the hazard model\n",
    "def evaluate_hazard_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds_hazard = []\n",
    "    all_labels_hazard = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            hazard_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            preds_hazard = torch.argmax(hazard_logits, dim=1).cpu().numpy()\n",
    "            all_preds_hazard.extend(preds_hazard)\n",
    "            all_labels_hazard.extend(batch[\"labels_hazard\"].cpu().numpy())\n",
    "    \n",
    "    return all_labels_hazard, all_preds_hazard\n",
    "\n",
    "# Evaluate product and hazard models separately\n",
    "y_new_test_product, all_preds_product = evaluate_product_model(product_model, product_new_test_loader)\n",
    "y_new_test_hazard, all_preds_hazard = evaluate_hazard_model(hazard_model, hazard_new_test_loader)\n",
    "\n",
    "# Compute custom F1 score\n",
    "new_score = compute_score(y_new_test_hazard, y_new_test_product, all_preds_hazard, all_preds_product)\n",
    "print(f\"Custom F1 Score on New Test Data: {new_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb28d6b-d262-4618-9e3e-1b7ccb852d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
