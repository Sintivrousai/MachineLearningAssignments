{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ce4a14-ab1d-48f2-92a3-3784d719256e",
   "metadata": {},
   "source": [
    "### 3rd assignment-ST2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efc2cf-3b5b-4c41-a52a-688cb42ce298",
   "metadata": {},
   "source": [
    "This is an implementation of Random Forest in order to predict the hazard and product based on the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eae144-797a-4b54-af82-02df83b661a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess data\n",
    "df_train = pd.read_csv(\"Data/incidents_train.csv\").dropna(subset=[\"title\", \"product\", \"hazard\"])\n",
    "df_valid = pd.read_csv(\"Data/incidents_valid.csv\").dropna(subset=[\"title\", \"product\", \"hazard\"])\n",
    "\n",
    "# Prepare TF-IDF vectorizer for the titles\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features (terms)\n",
    "\n",
    "# Fit the vectorizer on the training data (for title column)\n",
    "X_train = vectorizer.fit_transform(df_train['title'])\n",
    "X_valid = vectorizer.transform(df_valid['title'])\n",
    "\n",
    "# Prepare target variables for both products and hazards\n",
    "y_train_product = df_train['product']\n",
    "y_train_hazard = df_train['hazard']\n",
    "y_valid_product = df_valid['product']\n",
    "y_valid_hazard = df_valid['hazard']\n",
    "\n",
    "# Train Random Forest classifiers for both product and hazard prediction\n",
    "# For product classification\n",
    "product_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "product_rf.fit(X_train, y_train_product)\n",
    "\n",
    "# For hazard classification\n",
    "hazard_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "hazard_rf.fit(X_train, y_train_hazard)\n",
    "\n",
    "# Predict using both models\n",
    "y_pred_product = product_rf.predict(X_valid)\n",
    "y_pred_hazard = hazard_rf.predict(X_valid)\n",
    "\n",
    "# Compute accuracy and F1 score for both product and hazard\n",
    "product_accuracy = accuracy_score(y_valid_product, y_pred_product)\n",
    "hazard_accuracy = accuracy_score(y_valid_hazard, y_pred_hazard)\n",
    "\n",
    "f1_product = f1_score(y_valid_product, y_pred_product, average='macro')\n",
    "f1_hazard = f1_score(y_valid_hazard, y_pred_hazard, average='macro')\n",
    "\n",
    "# Display the results\n",
    "print(f\"Product Accuracy: {product_accuracy:.4f}\")\n",
    "print(f\"Hazard Accuracy: {hazard_accuracy:.4f}\")\n",
    "print(f\"Product F1 Score: {f1_product:.4f}\")\n",
    "print(f\"Hazard F1 Score: {f1_hazard:.4f}\")\n",
    "\n",
    "# If you want a combined score for both, you can compute it like this:\n",
    "combined_f1 = (f1_product + f1_hazard) / 2\n",
    "print(f\"Combined F1 Score: {combined_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30eeab-49be-4edb-be4c-977cd0327c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"Data/incidents_test.csv\").dropna(subset=[\"title\", \"product\", \"hazard\"])\n",
    "\n",
    "# Transform test data using the same vectorizer\n",
    "X_test = vectorizer.transform(df_test['title'])\n",
    "\n",
    "# Prepare test target variables\n",
    "y_test_product = df_test['product']\n",
    "y_test_hazard = df_test['hazard']\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_product_test = product_rf.predict(X_test)\n",
    "y_pred_hazard_test = hazard_rf.predict(X_test)\n",
    "\n",
    "# Compute accuracy and F1 score for both product and hazard\n",
    "product_accuracy = accuracy_score(y_test_product, y_pred_product_test)\n",
    "hazard_accuracy = accuracy_score(y_test_hazard, y_pred_hazard_test)\n",
    "\n",
    "f1_product = f1_score(y_test_product, y_pred_product_test, average='macro')\n",
    "f1_hazard = f1_score(y_test_hazard, y_pred_hazard_test, average='macro')\n",
    "\n",
    "# Print final evaluation results\n",
    "print(\"\\n--- Final Test Set Evaluation ---\")\n",
    "print(f\"Product Accuracy: {product_accuracy:.4f}\")\n",
    "print(f\"Hazard Accuracy: {hazard_accuracy:.4f}\")\n",
    "print(f\"Product F1 Score: {f1_product:.4f}\")\n",
    "print(f\"Hazard F1 Score: {f1_hazard:.4f}\")\n",
    "\n",
    "# Compute combined F1 score for overall performance\n",
    "combined_f1 = (f1_product + f1_hazard) / 2\n",
    "print(f\"Combined F1 Score: {combined_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32909039-eec9-44cb-bbe6-158d5a8d1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store test predictions and evaluation metrics\n",
    "df_predictions = pd.DataFrame({\n",
    "    \"Title\": df_test[\"title\"],\n",
    "    \"Predicted_Product\": y_pred_product_test,\n",
    "    \"Predicted_Hazard\": y_pred_hazard_test\n",
    "})\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "df_predictions.to_csv(\"submission_st2.csv\", index=False)\n",
    "print(\"Test predictions saved to 'submission_st2.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96f0ce-a50f-4be1-87cc-daf771114785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
