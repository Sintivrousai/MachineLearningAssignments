{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ce4a14-ab1d-48f2-92a3-3784d719256e",
   "metadata": {},
   "source": [
    "# 3rd assignment-ST1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a609993-e70f-416b-b3d1-0df1313874c4",
   "metadata": {},
   "source": [
    "In case you don't have the libraries below installed, please install them and afterwards make the commands comments to avoid reinstallation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21db412b-7e24-4126-a735-9cb1de723aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.48.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.4)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: tiktoken in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
      "Requirement already satisfied: sentencepiece==0.1.96 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.1.96)\n",
      "Requirement already satisfied: focal-loss-torch in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from focal-loss-torch) (2.6.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from focal-loss-torch) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (3.4)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch->focal-loss-torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch->focal-loss-torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch->focal-loss-torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install tiktoken\n",
    "!pip install sentencepiece==0.1.96\n",
    "!pip install focal-loss-torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89de668-c25b-4589-b9cb-9d43c67bad50",
   "metadata": {},
   "source": [
    "Below we are importing all the libraries that are needed for the development. Afterwards, we are importing 2 files, one for training and one for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18a1d527-df07-4bad-8369-1ecf4f3b4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# NLTK (Natural Language Toolkit)\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# PyTorch & Deep Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Transformers (Hugging Face)\n",
    "from transformers import BertTokenizer, BertModel, DebertaV2Tokenizer\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Tokenization & Preprocessing Libraries\n",
    "import tiktoken\n",
    "import sentencepiece\n",
    "\n",
    "# Custom Loss Function\n",
    "from focal_loss import FocalLoss\n",
    "\n",
    "\n",
    "# Load train and validation datasets )\n",
    "df_train = pd.read_csv(\"Data/incidents_train.csv\", encoding='utf-8')\n",
    "df_valid = pd.read_csv(\"Data/incidents_valid.csv\", encoding='utf-8')\n",
    "\n",
    "# Prepare the training data\n",
    "df_train = df_train.dropna(subset=[\"title\", \"product-category\", \"hazard-category\"])\n",
    "df_train[\"product-category\"] = df_train[\"product-category\"].astype(\"category\").cat.codes\n",
    "df_train[\"hazard-category\"] = df_train[\"hazard-category\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Prepare the validation data\n",
    "df_valid = df_valid.dropna(subset=[\"title\", \"product-category\", \"hazard-category\"])\n",
    "df_valid[\"product-category\"] = df_valid[\"product-category\"].astype(\"category\").cat.codes\n",
    "df_valid[\"hazard-category\"] = df_valid[\"hazard-category\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Extract features and labels for training and validation sets\n",
    "X_train = df_train[\"title\"]\n",
    "y_train_product = df_train[\"product-category\"]\n",
    "y_train_hazard = df_train[\"hazard-category\"]\n",
    "\n",
    "X_test = df_valid[\"title\"]\n",
    "y_test_product = df_valid[\"product-category\"]\n",
    "y_test_hazard = df_valid[\"hazard-category\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d54bd7-366a-4041-9fc7-7d7191a6dbd9",
   "metadata": {},
   "source": [
    "Below, there are some methods for augmentation using synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5ec537d-b7f3-4ea9-a04e-dac44393e080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data (run once) Afterwards, add them as comment\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Synonym replacement function using WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "# Function for synonym augmentation\n",
    "def synonym_augmentation(text, aug_p=0.1):\n",
    "    words = text.split()\n",
    "    augmented_text = words[:]\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if random.random() < aug_p:  # Probability of replacing the word\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms:\n",
    "                augmented_text[i] = random.choice(synonyms)\n",
    "    \n",
    "    return \" \".join(augmented_text)\n",
    "class_counts_products = df_train['product-category'].value_counts() \n",
    "class_counts_hazards = df_train['hazard-category'].value_counts() \n",
    "\n",
    "minor_product =200\n",
    "minor_hazard=54\n",
    "under_represented_classes_product = class_counts_products[class_counts_products < minor_product].index.tolist() \n",
    "under_represented_classes_hazard = class_counts_hazards[class_counts_hazards < minor_hazard].index.tolist() \n",
    "\n",
    "minority_df_products = df_train[df_train[\"product-category\"].isin(under_represented_classes_product)] \n",
    "minority_df_hazards = df_train[df_train[\"hazard-category\"].isin(under_represented_classes_hazard)] \n",
    "max_class_size_product = 1434 \n",
    "max_class_size_hazard = 1854\n",
    "\n",
    "def compute_augmentation_factor(class_size, max_class_size): \n",
    "    return min(int(np.log2(max_class_size / class_size) + 1), 15)\n",
    "\n",
    "augmentation_factors_products={ \n",
    "    category: compute_augmentation_factor(count, max_class_size_product) \n",
    "    for category, count in class_counts_products.items() \n",
    "}\n",
    "augmentation_factors_hazards={ \n",
    "    category: compute_augmentation_factor(count, max_class_size_hazard) \n",
    "    for category, count in class_counts_hazards.items()\n",
    "}\n",
    "\n",
    "augmented_data_products = []\n",
    "for index, row in minority_df_products.iterrows(): \n",
    "    original_title = row['title'] \n",
    "    product_category = row['product-category'] \n",
    "    num_augmentations = augmentation_factors_products[product_category] \n",
    "    for _ in range(num_augmentations): \n",
    "        augmented_title=synonym_augmentation(original_title) \n",
    "        augmented_data_products.append({'title': augmented_title, 'product-category': product_category}) \n",
    "augmented_product_df = pd.DataFrame(augmented_data_products) \n",
    "augmented_product_df=augmented_product_df.dropna(subset=['title']) \n",
    "\n",
    "# Merge original and augmented product data\n",
    "df_train_product_augmented = pd.concat([df_train, augmented_product_df], ignore_index=True)\n",
    "\n",
    "# Extract updated X_train_product and y_train_product\n",
    "X_train_product = df_train_product_augmented[\"title\"]\n",
    "\n",
    "augmented_data_hazards = []\n",
    "for index, row in minority_df_hazards.iterrows(): \n",
    "    original_title = row['title'] \n",
    "    hazard_category = row['hazard-category'] \n",
    "    num_augmentations = augmentation_factors_hazards[hazard_category] \n",
    "    for _ in range(num_augmentations): \n",
    "        augmented_title=synonym_augmentation(original_title) \n",
    "        augmented_data_hazards.append({'title': augmented_title, 'hazard-category': hazard_category}) \n",
    "augmented_hazard_df = pd.DataFrame(augmented_data_hazards) \n",
    "augmented_hazard_df=augmented_hazard_df.dropna(subset=['title']) \n",
    "\n",
    "# Merge original and augmented product data\n",
    "df_train_hazard_augmented = pd.concat([df_train, augmented_hazard_df], ignore_index=True)\n",
    "\n",
    "# Extract updated X_train_product and y_train_product\n",
    "X_train_hazard = df_train_hazard_augmented[\"title\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a87ce-0ed7-4317-8e5d-478a79be9ee3",
   "metadata": {},
   "source": [
    "This code sets up a BERT model for both product and hazard prediction, tokenizes the text, and prepares labels as tensors for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c2ef08-f5b8-4bb9-a67c-c10ef99978f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Name (using BERT)\n",
    "MODEL_NAME = \"bert-base-uncased\"  # Change to BERT (you can use bert-large-uncased if you want a larger model)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move tokenized input to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def tokenize_text(texts):\n",
    "    encodings = tokenizer(list(texts), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    return encodings.to(device)\n",
    "\n",
    "# Tokenize texts for both models (product and hazard)\n",
    "train_encodings_product = tokenize_text(X_train_product)  # Tokenized inputs for product model\n",
    "train_encodings_hazard = tokenize_text(X_train_hazard)\n",
    "test_encodings = tokenize_text(X_test)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels_product = torch.tensor(y_train_product.values)\n",
    "train_labels_hazard = torch.tensor(y_train_hazard.values)\n",
    "test_labels_product = torch.tensor(y_test_product.values)\n",
    "test_labels_hazard = torch.tensor(y_test_hazard.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e843fee-a214-4823-936e-4b94f3ba751e",
   "metadata": {},
   "source": [
    "This code defines two separate BERT-based classification models—one for product classification and one for hazard classification—using PyTorch and Hugging Face’s BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd65b80-60f7-4cee-9a14-311d84286fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_product_labels):\n",
    "        super(ProductBERT, self).__init__()\n",
    "        # Change to BERT\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.product_classifier = nn.Linear(self.bert.config.hidden_size, num_product_labels)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # BERT outputs a BaseModelOutput object\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # Extract last hidden state\n",
    "        cls_hidden_state = hidden_states[:, 0, :]  # Get [CLS] token representation\n",
    "        cls_hidden_state = self.dropout(cls_hidden_state)  # Apply dropout\n",
    "        product_logits = self.product_classifier(cls_hidden_state)  # Apply classifier on [CLS] token\n",
    "        return product_logits  # Return the logits directly\n",
    "        \n",
    "class HazardBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_hazard_labels):\n",
    "        super(HazardBERT, self).__init__()\n",
    "        # Change to BERT\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.hazard_classifier = nn.Linear(self.bert.config.hidden_size, num_hazard_labels)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        cls_hidden_state = hidden_states[:, 0, :]  # [CLS] token\n",
    "        cls_hidden_state = self.dropout(cls_hidden_state)\n",
    "        hazard_logits = self.hazard_classifier(cls_hidden_state)\n",
    "        return hazard_logits\n",
    "\n",
    "# Get number of classes for each task\n",
    "num_product_labels = len(df_train[\"product-category\"].unique())\n",
    "num_hazard_labels = len(df_train[\"hazard-category\"].unique())\n",
    "\n",
    "# Initialize both models separately using BERT instead of DeBERTa\n",
    "product_model = ProductBERT(\"bert-base-uncased\", num_product_labels)  # Use BERT model\n",
    "hazard_model = HazardBERT(\"bert-base-uncased\", num_hazard_labels)  # Use BERT model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f11f8c-6156-4496-9b3f-d9a6d9d34fab",
   "metadata": {},
   "source": [
    "This code prepares the dataset and data loaders for training and evaluating the BERT-based product and hazard classification models using PyTorch’s Dataset and DataLoader classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c24d5568-b94a-4de5-b8a4-843cc5e2883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset for product task\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, encodings, labels_product):\n",
    "        self.encodings = encodings\n",
    "        self.labels_product = labels_product\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_product)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels_product\"] = self.labels_product[idx]\n",
    "        return item\n",
    "\n",
    "# Create Dataset for hazard task\n",
    "class HazardDataset(Dataset):\n",
    "    def __init__(self, encodings, labels_hazard):\n",
    "        self.encodings = encodings\n",
    "        self.labels_hazard = labels_hazard\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_hazard)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels_hazard\"] = self.labels_hazard[idx]\n",
    "        return item\n",
    "\n",
    "# Create WeightedRandomSampler for the product category\n",
    "#weighted_sampler_product = WeightedRandomSampler(weights=sample_weights_product, num_samples=len(sample_weights_product), replacement=True)\n",
    "\n",
    "# Create WeightedRandomSampler for the hazard category (optional to apply to both)\n",
    "#weighted_sampler_hazard = WeightedRandomSampler(weights=sample_weights_hazard, num_samples=len(sample_weights_hazard), replacement=True)\n",
    "\n",
    "# Create Dataset for product classification task\n",
    "product_train_dataset = ProductDataset(train_encodings_product, train_labels_product)\n",
    "product_test_dataset = ProductDataset(test_encodings, test_labels_product)\n",
    "\n",
    "# Create Dataset for hazard classification task\n",
    "hazard_train_dataset = HazardDataset(train_encodings_hazard, train_labels_hazard)\n",
    "hazard_test_dataset = HazardDataset(test_encodings, test_labels_hazard)\n",
    "\n",
    "# Create DataLoader for the product classification task\n",
    "product_train_loader = DataLoader(product_train_dataset, batch_size=16, shuffle=True)\n",
    "product_test_loader = DataLoader(product_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Create DataLoader for the hazard classification task\n",
    "hazard_train_loader = DataLoader(hazard_train_dataset, batch_size=16,  shuffle=True)\n",
    "hazard_test_loader = DataLoader(hazard_test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc8dee-de79-4b52-8e96-9af873205125",
   "metadata": {},
   "source": [
    "This code trains two separate BERT-based models—one for product classification and one for hazard classification—using a custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b4585-13a1-44b8-a0d5-beb51b7244de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20865/427602341.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Product Model Loss: 1.3546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20865/427602341.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Hazard Model Loss: 0.5448\n",
      "Epoch 2 | Product Model Loss: 0.6378\n"
     ]
    }
   ],
   "source": [
    "# Assuming product_class_weights and hazard_class_weights are tensors\n",
    "loss_fn_product = FocalLoss(gamma=2, reduction='mean')\n",
    "loss_fn_hazard = FocalLoss(gamma=2, reduction='mean')\n",
    "\n",
    "# Separate optimizers for product and hazard models\n",
    "optimizer_product = AdamW(product_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "optimizer_hazard = AdamW(hazard_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler for both models (if necessary)\n",
    "num_training_steps_product = len(product_train_loader) * 5  # 5 epochs for product model\n",
    "num_training_steps_hazard = len(hazard_train_loader) * 5  # 5 epochs for hazard model\n",
    "lr_scheduler_product = get_scheduler(\"linear\", optimizer=optimizer_product, num_warmup_steps=0, num_training_steps=num_training_steps_product)\n",
    "lr_scheduler_hazard = get_scheduler(\"linear\", optimizer=optimizer_hazard, num_warmup_steps=0, num_training_steps=num_training_steps_hazard)\n",
    "\n",
    "product_model = product_model.to(device)\n",
    "hazard_model = hazard_model.to(device)\n",
    "\n",
    "# Training Loop for both models\n",
    "epochs = 6\n",
    "for epoch in range(epochs):\n",
    "    # Train Product Model\n",
    "    product_model.train()\n",
    "    total_loss_product = 6\n",
    "    for batch in product_train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}  # Move batch items to the correct device\n",
    "\n",
    "        optimizer_product.zero_grad()  # Zero gradients for product model\n",
    "\n",
    "        # Forward pass for product model\n",
    "        product_logits = product_model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "\n",
    "        product_probs = F.softmax(product_logits, dim=-1)  # For product classification\n",
    "\n",
    "        # Ensure labels are in LongTensor format\n",
    "        batch[\"labels_product\"] = batch[\"labels_product\"].to(device).long()\n",
    "\n",
    "        # Compute loss for product model\n",
    "        loss_product = loss_fn_product(product_probs, batch[\"labels_product\"])\n",
    "\n",
    "        # Backpropagation for product model\n",
    "        loss_product.backward()\n",
    "        optimizer_product.step()\n",
    "\n",
    "        # Step learning rate scheduler\n",
    "        lr_scheduler_product.step()\n",
    "\n",
    "        total_loss_product += loss_product.item()\n",
    "\n",
    "    # Average loss for product model after each epoch\n",
    "    avg_loss_product = total_loss_product / len(product_train_loader)\n",
    "    print(f\"Epoch {epoch+1} | Product Model Loss: {avg_loss_product:.4f}\")\n",
    "\n",
    "    # Train Hazard Model\n",
    "    hazard_model.train()\n",
    "    total_loss_hazard = 0\n",
    "    for batch in hazard_train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}  # Move batch items to the correct device\n",
    "\n",
    "        optimizer_hazard.zero_grad()  # Zero gradients for hazard model\n",
    "\n",
    "        # Forward pass for hazard model\n",
    "        hazard_logits = hazard_model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "\n",
    "        hazard_probs = F.softmax(hazard_logits, dim=-1)    # For hazard classification\n",
    "\n",
    "        # Ensure labels are in LongTensor format\n",
    "        batch[\"labels_hazard\"] = batch[\"labels_hazard\"].to(device).long()\n",
    "\n",
    "        # Compute loss for hazard model\n",
    "        loss_hazard = loss_fn_hazard(hazard_probs, batch[\"labels_hazard\"])\n",
    "\n",
    "        # Backpropagation for hazard model\n",
    "        loss_hazard.backward()\n",
    "        optimizer_hazard.step()\n",
    "\n",
    "        # Step learning rate scheduler\n",
    "        lr_scheduler_hazard.step()\n",
    "\n",
    "        total_loss_hazard += loss_hazard.item()\n",
    "\n",
    "    # Average loss for hazard model after each epoch\n",
    "    avg_loss_hazard = total_loss_hazard / len(hazard_train_loader)\n",
    "    print(f\"Epoch {epoch+1} | Hazard Model Loss: {avg_loss_hazard:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b9c77-a5e2-4f19-a9f7-d358c4155de8",
   "metadata": {},
   "source": [
    "This part evaluates the model based on the validation dataset. It evaluates for production model and hazard model separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c93732-6d20-471f-a0c0-0d10c9ae22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_product_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds_product = []\n",
    "    all_labels_product = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            product_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            preds_product = torch.argmax(product_logits, dim=1).cpu().numpy()\n",
    "            all_preds_product.extend(preds_product)\n",
    "            all_labels_product.extend(batch[\"labels_product\"].cpu().numpy())\n",
    "    \n",
    "    print(\"Product Category Classification Report:\")\n",
    "    print(classification_report(all_labels_product, all_preds_product))\n",
    "    return all_labels_product, all_preds_product\n",
    "\n",
    "def evaluate_hazard_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds_hazard = []\n",
    "    all_labels_hazard = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            hazard_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            preds_hazard = torch.argmax(hazard_logits, dim=1).cpu().numpy()\n",
    "            all_preds_hazard.extend(preds_hazard)\n",
    "            all_labels_hazard.extend(batch[\"labels_hazard\"].cpu().numpy())\n",
    "    \n",
    "    print(\"Hazard Category Classification Report:\")\n",
    "    print(classification_report(all_labels_hazard, all_preds_hazard))\n",
    "    return all_labels_hazard, all_preds_hazard\n",
    "\n",
    "# Evaluate the product and hazard models separately\n",
    "y_test_product, all_preds_product = evaluate_product_model(product_model, product_test_loader)\n",
    "y_test_hazard, all_preds_hazard = evaluate_hazard_model(hazard_model, hazard_test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0332da-276c-4efa-89a5-6cbb9bd86d98",
   "metadata": {},
   "source": [
    "This part calculates f1 score according to the assignments evaluation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff057d-e271-4bb0-a99b-0979776621ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(hazards_true, products_true, hazards_pred, products_pred):\n",
    "    # Convert to NumPy arrays for Boolean indexing\n",
    "    hazards_true = np.array(hazards_true)\n",
    "    products_true = np.array(products_true)\n",
    "    hazards_pred = np.array(hazards_pred)\n",
    "    products_pred = np.array(products_pred)\n",
    "\n",
    "    # Compute F1-score for hazards\n",
    "    f1_hazards = f1_score(hazards_true, hazards_pred, average='macro')\n",
    "\n",
    "    # Compute F1-score for products (only where hazard prediction is correct)\n",
    "    correct_hazard_mask = hazards_pred == hazards_true\n",
    "    f1_products = f1_score(\n",
    "        products_true[correct_hazard_mask], \n",
    "        products_pred[correct_hazard_mask], \n",
    "        average='macro'\n",
    "    ) if np.any(correct_hazard_mask) else 0  # Handle case where no correct hazards\n",
    "\n",
    "    # Compute final score\n",
    "    return (f1_hazards + f1_products) / 2.\n",
    "\n",
    "score = compute_score(y_test_hazard, y_test_product, all_preds_hazard, all_preds_product)\n",
    "print(f\"Custom F1 Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c5fe2-0882-4196-b1e2-65c415b46b7c",
   "metadata": {},
   "source": [
    "This is the evaluation of the test dataset and the f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a5fca9-00a7-4c34-81e8-f3ad895fa79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new test file (adjust file path)\n",
    "new_test_df = pd.read_csv(\"Data/incidents_test.csv\")  # Replace with your actual file\n",
    "\n",
    "# Ensure it has the same preprocessing\n",
    "new_test_df = new_test_df.dropna(subset=[\"title\", \"product-category\", \"hazard-category\"])\n",
    "new_test_df[\"product-category\"] = new_test_df[\"product-category\"].astype(\"category\").cat.codes\n",
    "new_test_df[\"hazard-category\"] = new_test_df[\"hazard-category\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Tokenize new test set\n",
    "new_test_encodings = tokenize_text(new_test_df[\"title\"])\n",
    "\n",
    "# Convert labels to tensors\n",
    "new_test_labels_product = torch.tensor(new_test_df[\"product-category\"].values)\n",
    "new_test_labels_hazard = torch.tensor(new_test_df[\"hazard-category\"].values)\n",
    "\n",
    "# Create Dataset for product classification task\n",
    "product_new_test_dataset = ProductDataset(new_test_encodings, new_test_labels_product)\n",
    "\n",
    "# Create Dataset for hazard classification task\n",
    "hazard_new_test_dataset = HazardDataset(new_test_encodings, new_test_labels_hazard)\n",
    "\n",
    "# Create DataLoader for the product classification task\n",
    "product_new_test_loader = DataLoader(product_new_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Create DataLoader for the hazard classification task\n",
    "hazard_new_test_loader = DataLoader(hazard_new_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Evaluate the product model\n",
    "def evaluate_product_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds_product = []\n",
    "    all_labels_product = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            product_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            preds_product = torch.argmax(product_logits, dim=1).cpu().numpy()\n",
    "            all_preds_product.extend(preds_product)\n",
    "            all_labels_product.extend(batch[\"labels_product\"].cpu().numpy())\n",
    "\n",
    "    print(\"Product Category Classification Report:\")\n",
    "    print(classification_report(all_labels_product, all_preds_product))\n",
    "    return all_labels_product, all_preds_product\n",
    "\n",
    "    \n",
    "    return all_labels_product, all_preds_product\n",
    "\n",
    "# Evaluate the hazard model\n",
    "def evaluate_hazard_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds_hazard = []\n",
    "    all_labels_hazard = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            hazard_logits = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            preds_hazard = torch.argmax(hazard_logits, dim=1).cpu().numpy()\n",
    "            all_preds_hazard.extend(preds_hazard)\n",
    "            all_labels_hazard.extend(batch[\"labels_hazard\"].cpu().numpy())\n",
    "    \n",
    "    print(\"Hazard Category Classification Report:\")\n",
    "    print(classification_report(all_labels_hazard, all_preds_hazard))\n",
    "    return all_labels_hazard, all_preds_hazard\n",
    "\n",
    "    \n",
    "    return all_labels_hazard, all_preds_hazard\n",
    "\n",
    "# Evaluate product and hazard models separately\n",
    "y_new_test_product, all_preds_product = evaluate_product_model(product_model, product_new_test_loader)\n",
    "y_new_test_hazard, all_preds_hazard = evaluate_hazard_model(hazard_model, hazard_new_test_loader)\n",
    "\n",
    "# Compute custom F1 score\n",
    "new_score = compute_score(y_new_test_hazard, y_new_test_product, all_preds_hazard, all_preds_product)\n",
    "print(f\"Custom F1 Score on New Test Data: {new_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb28d6b-d262-4618-9e3e-1b7ccb852d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
